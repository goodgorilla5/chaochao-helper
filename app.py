import streamlit as st
import pandas as pd
import re
import requests

st.set_page_config(page_title="ç‡•å·¢è¡Œæƒ…(è‡ªå‹•åŒæ­¥ç‰ˆ)", layout="centered")

def process_logic(content):
    final_rows = []
    grade_map = {"1": "ç‰¹", "2": "å„ª", "3": "è‰¯"}
    # ä½¿ç”¨ F22 ä½œç‚ºåˆ‡å‰²æ¨™è¨˜
    parts = content.split('F22')
    for p in parts[1:]:
        if "S00076" in p:
            try:
                # å°‹æ‰¾æ—¥æœŸåº§æ¨™å®šä½
                date_match = re.search(r"(\d{7,8}1)", p)
                if not date_match: continue
                date_pos = date_match.start()
                # åˆä½µæµæ°´è™Ÿ
                serial = p[:date_pos].strip().replace(" ", "")
                s_pos = p.find("S00076")
                # ç­‰ç´šèˆ‡å°ä»£
                level = grade_map.get(p[s_pos-2], p[s_pos-2])
                sub_id = p[s_pos+6:s_pos+9]
                # æ•¸å­—å€è™•ç†
                nums = p.split('+')
                pieces = int(nums[0][-3:].lstrip('0') or 0)
                weight = int(nums[1].lstrip('0') or 0)
                price = int(nums[2].lstrip('0')[:-1] or 0)
                buyer = nums[5].strip()[:4]

                final_rows.append({
                    "ç­‰ç´š": level, "å°ä»£": sub_id, "ä»¶æ•¸": pieces, 
                    "å…¬æ–¤": weight, "å–®åƒ¹": price, "è²·å®¶": buyer, "æµæ°´è™Ÿ": serial
                })
            except: continue
    return final_rows

st.title("ğŸ ç‡•å·¢-å°åŒ—ç¾å ´å°å¸³")

# --- å˜—è©¦è®€å– GitHub è‡ªå‹•æ›´æ–°æª” ---
RAW_URL = "https://raw.githubusercontent.com/goodgorilla5/chaochao-helper/main/today.scp"

@st.cache_data(ttl=600)
def fetch_auto_data():
    try:
        r = requests.get(RAW_URL, timeout=5)
        if r.status_code == 200 and len(r.text) > 100:
            return r.text
    except: return None
    return None

auto_content = fetch_auto_data()
final_content = None

# ä»‹é¢é‚è¼¯ï¼šå¦‚æœ GitHub æœ‰è³‡æ–™å°±é¡¯ç¤ºï¼Œä¸¦æä¾›ã€Œæ‰‹å‹•è¦†è“‹ã€æŒ‰éˆ•
if auto_content:
    st.success("âœ… å·²è‡ªå‹•è¼‰å…¥ä»Šæ—¥é›²ç«¯è³‡æ–™")
    with st.expander("å¦‚æœ‰éœ€è¦ï¼Œå¯æ‰‹å‹•ä¸Šå‚³æª”æ¡ˆè¦†è“‹"):
        manual_file = st.file_uploader("ä¸Šå‚³æ–°çš„ SCP æª”æ¡ˆ", type=['scp', 'txt'])
        if manual_file:
            final_content = manual_file.read().decode("big5", errors="ignore")
    if not final_content:
        final_content = auto_content
else:
    st.warning("âš ï¸ é›²ç«¯ç›®å‰ç„¡è³‡æ–™ï¼Œè«‹é»æ“Šä¸‹æ–¹æ‰‹å‹•ä¸Šå‚³ã€‚")
    manual_file = st.file_uploader("ğŸ“‚ æ‰‹å‹•ä¸Šå‚³ SCP æª”æ¡ˆ", type=['scp', 'txt'])
    if manual_file:
        final_content = manual_file.read().decode("big5", errors="ignore")

# --- é¡¯ç¤ºçµæœ ---
if final_content:
    data = process_logic(final_content)
    if data:
        df = pd.DataFrame(data)
        st.divider()
        search_query = st.text_input("ğŸ” æŸ¥è©¢å°ä»£è™Ÿ", "")
        if search_query:
            df = df[df['å°ä»£'].str.contains(search_query)]
        
        # é è¨­å–®åƒ¹ç”±é«˜åˆ°ä½
        df = df.sort_values(by="å–®åƒ¹", ascending=False)
        
        # é¡¯ç¤ºæ¬„ä½
        st.dataframe(df[["ç­‰ç´š", "å°ä»£", "ä»¶æ•¸", "å–®åƒ¹", "è²·å®¶"]], use_container_width=True, height=500)
        
        col1, col2 = st.columns(2)
        col1.metric("ç¸½ä»¶æ•¸", f"{df['ä»¶æ•¸'].sum()} ä»¶")
        col2.write(f"è³‡æ–™æ›´æ–°æ™‚é–“: {pd.Timestamp.now(tz='Asia/Taipei').strftime('%H:%M')}")