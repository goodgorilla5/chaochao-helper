import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import datetime
import time

st.set_page_config(page_title="ç‡•å·¢å°åŒ—å°å¸³-å¼·éŸŒç‰ˆ", layout="wide")

# è§£æé‚è¼¯ä¿æŒä¸è®Š...
def parse_scp_content(content):
    final_rows = []
    lines = content.split('\n')
    for line in lines:
        if "F22" in line:
            parts = line.replace('+', ' ').split()
            try:
                final_rows.append({
                    "å°ä»£": str(parts[3])[-3:],
                    "ä»¶æ•¸": int(parts[5].lstrip('0') or 0),
                    "å…¬æ–¤": int(parts[6].lstrip('0') or 0),
                    "å–®åƒ¹": int(parts[7].lstrip('0')[:-1] or 0),
                    "è²·å®¶": parts[-1]
                })
            except: continue
    return final_rows

@st.cache_data(ttl=600) # ç¸®çŸ­å¿«å–åˆ°10åˆ†é˜ï¼Œç¢ºä¿æ•¸æ“šå¤ æ–°
def get_latest_data():
    url = "https://amis.afa.gov.tw/download/DownloadVegFruitCoopData2.aspx"
    # æ›´å®Œæ•´çš„ Headerï¼Œå½è£æˆä¸€èˆ¬çš„ Chrome ç€è¦½å™¨
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7",
        "Referer": "https://amis.afa.gov.tw/"
    }
    
    session = requests.Session()
    max_retries = 3
    
    for attempt in range(max_retries):
        try:
            # 1. ç²å–é€šè¡Œè­‰ï¼Œå¢åŠ  timeout åˆ° 30 ç§’
            res = session.get(url, headers=headers, timeout=30)
            res.raise_for_status()
            soup = BeautifulSoup(res.text, 'html.parser')
            
            payload = {
                "__VIEWSTATE": soup.find("input", {"id": "__VIEWSTATE"})['value'],
                "__VIEWSTATEGENERATOR": soup.find("input", {"id": "__VIEWSTATEGENERATOR"})['value'],
                "__EVENTVALIDATION": soup.find("input", {"id": "__EVENTVALIDATION"})['value'],
                "ctl00$contentPlaceHolder$txtSupplyNo": "S00076 ç‡•å·¢å€è¾²æœƒ",
                "ctl00$contentPlaceHolder$hfldSupplyNo": "S00076",
                "ctl00$contentPlaceHolder$btnQuery2": "4ç¢¼å“åä»£ç¢¼" 
            }
            
            # 2. è«‹æ±‚æ•¸æ“š
            post_res = session.post(url, data=payload, headers=headers, timeout=30)
            if "F22" in post_res.text:
                return parse_scp_content(post_res.text)
            else:
                return [] # æ²’è³‡æ–™ä½†ä¸ç®—éŒ¯èª¤
                
        except (requests.exceptions.RequestException, Exception) as e:
            if attempt < max_retries - 1:
                time.sleep(2) # å¤±æ•—å¾Œç­‰ 2 ç§’å†è©¦
                continue
            return f"é€£ç·šè¾²å§”æœƒè¶…æ™‚ï¼Œè«‹æª¢æŸ¥ç¶²è·¯æˆ–ç¨å¾Œå†è©¦ã€‚åŸå› : {e}"

# --- ä¸»ç¨‹å¼ ---
st.title("ğŸ ç‡•å·¢è¾²æœƒ - å°åŒ—å°å¸³è‡ªå‹•çœ‹æ¿")

# æä¾›æ‰‹å‹•åˆ·æ–°çš„æŒ‰éˆ•ï¼ˆä»¥é˜²å¿«å–æ²’æ›´æ–°ï¼‰
if st.sidebar.button("ğŸ”„ å¼·åˆ¶é‡æ–°æ•´ç†æ•¸æ“š"):
    st.cache_data.clear()
    st.rerun()

with st.spinner("ğŸš€ æ­£åœ¨åŠªåŠ›ç©¿é€ç¶²è·¯é€£ç·šè‡³è¾²å§”æœƒ..."):
    data = get_latest_data()

# (å¾ŒçºŒé¡¯ç¤ºé‚è¼¯åŒå‰ä¸€ç‰ˆ...)
if isinstance(data, list):
    if data:
        df = pd.DataFrame(data).sort_values(by="å–®åƒ¹", ascending=False)
        # é¡¯ç¤ºæŒ‡æ¨™...
        t1, t2, t3 = st.columns(3)
        t1.metric("ä»Šæ—¥ç¸½ä»¶æ•¸", f"{df['ä»¶æ•¸'].sum()} ä»¶")
        t2.metric("æœ€é«˜åƒ¹", f"{df['å–®åƒ¹'].max()} å…ƒ")
        t3.metric("ç¸½å…¬æ–¤", f"{df['å…¬æ–¤'].sum()}")
        st.divider()
        st.dataframe(df, use_container_width=True, height=600)
    else:
        st.warning("âš ï¸ ç›®å‰è¾²å§”æœƒå°šæœªç”¢å‡ºä»Šæ—¥è³‡æ–™ï¼ˆè«‹æ–¼ä¸­åˆå·¦å³æŸ¥çœ‹ï¼‰ã€‚")
else:
    st.error(data)